{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Working with N-Grams"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import nltk\n",
                "import re\n",
                "import os\n",
                "from pathlib import Path\n",
                "from nltk.lm.preprocessing import pad_both_ends, padded_everygrams\n",
                "from nltk.util import everygrams\n",
                "from nltk.util import ngrams\n",
                "from itertools import groupby\n",
                "from nltk.lm import MLE\n",
                "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
                "import numpy as np\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# navigate folders\n",
                "p=Path(os.getcwd())\n",
                "os.chdir(p.parent)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "os.getcwd()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'/Users/Chris/Documents/00.Data_science/00.MADS/GitHub_MADS/portfolio'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# import data\n",
                "nltk_data_path = \"assets/nltk_data\"\n",
                "if nltk_data_path not in nltk.data.path:\n",
                "    nltk.data.path.append(nltk_data_path)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create a few functions to help parse the data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def load_data():\n",
                "    \"\"\"\n",
                "    Load text data from a file and produce a list of token lists\n",
                "    \"\"\"\n",
                "    \n",
                "    with open('assets/gutenberg/THE_SONNETS.txt', \"r\") as file_object:\n",
                "        \n",
                "        # read file content\n",
                "        data = file_object.readlines()\n",
                "        data = [re.sub(r'\\d+','',li) for li in data] # removing numbers\n",
                "        \n",
                "        sentences = [line.lstrip() for line in data] # removing leading space\n",
                "        sentences = [line.strip('\\n') for line in sentences] # removing line break\n",
                "        sentences = list(filter(None, sentences)) # removing empty lists\n",
                "        sentences = [nltk.word_tokenize(w.lower()) for w in sentences] # tokenizing words\n",
                "    \n",
                "    return sentences"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "def build_vocab(sentences):\n",
                "    \"\"\"\n",
                "    Take a list of sentences and return a vocab\n",
                "    \"\"\"\n",
                "    words = set()\n",
                "    for sent in sentences:\n",
                "        words.update(set(sent))\n",
                "    words.update(('<s>', '</s>'))\n",
                "    vocab = list(words)\n",
                "    \n",
                "    return vocab"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "def build_ngrams(n, sentences):\n",
                "    \"\"\"\n",
                "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
                "    \"\"\"\n",
                "    sentences_d = [list(pad_both_ends(sent, n)) for sent in sentences]\n",
                "    all_ngrams = [list(ngrams(sent, n)) for sent in sentences_d]\n",
                "    \n",
                "    return all_ngrams"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# run functions\n",
                "sentence_data = load_data()\n",
                "vocab = build_vocab(sentence_data)\n",
                "bigrams = build_ngrams(2, sentence_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Let's now show that we can guess the next token \n",
                "### First-Order Markov"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "# bigram occurences\n",
                "bigram_occurences = [grams[0] for grams in bigrams]\n",
                "\n",
                "# create key set with implied <s> start\n",
                "bigram_set = set(bigram_occurences)\n",
                "# create count\n",
                "start_count = len(bigram_occurences)\n",
                "# instantiate dict\n",
                "bi_dict = {}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
                "    \"\"\"\n",
                "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
                "    \"\"\"\n",
                "    \n",
                "    for bi in bigram_set:\n",
                "        bi_dict[bi] = bigram_occurences.count(bi)/start_count\n",
                "\n",
                "    keymax = max(bi_dict, key=bi_dict.get)\n",
                "\n",
                "    next_token, prob = keymax[1], bi_dict[keymax]\n",
                "    \n",
                "    \n",
                "    return next_token, prob"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "# okay let's test this function\n",
                "bigram_next_token(start_tokens=(\"<s>\", ) * 3)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "('and', 0.1122969837587007)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 11
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Train an N-Gram language model\n",
                "Now we are well positioned to start training an $n$-gram language model. We can fit a language model using the `MLE` class from `nltk.lm`. It requires two inputs: a list of all $n$-grams for each sentence and a vocabulary, both of which you have already written a function to build. Now it's time to put them together to work. "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def train_ngram_lm(n):\n",
                "    \"\"\"\n",
                "    Train a n-gram language model as specified by the argument \"n\"\n",
                "    This uses the global sentence_data variable\n",
                "    \"\"\"\n",
                "    ngrams = build_ngrams(n, sentence_data)\n",
                "    lm = MLE(n)\n",
                "    lm.fit(ngrams, vocabulary_text=vocab)\n",
                "    \n",
                "    return lm"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Okay now let's have some fun and generate a Shakespearean poem from a corpus of his work"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "# Every time it runs, depending on how drunk it is, a different sonnet is written. \n",
                "n = 3\n",
                "num_lines = 14\n",
                "num_words_per_line = 8\n",
                "text_seed = [\"<s>\"] * (n - 1)\n",
                "\n",
                "lm = train_ngram_lm(n)\n",
                "\n",
                "sonnet = []\n",
                "while len(sonnet) < num_lines:\n",
                "    while True:  # keep generating a line until success\n",
                "        try:\n",
                "            line = lm.generate(num_words_per_line, text_seed=text_seed)\n",
                "        except ValueError:  # the generation is not always successful. need to capture exceptions\n",
                "            continue\n",
                "        else:\n",
                "            line = [x for x in line if x not in [\"<s>\", \"</s>\"]]\n",
                "            sonnet.append(\" \".join(line))\n",
                "            break\n",
                "\n",
                "# pretty-print your sonnet\n",
                "print(\"\\n\".join(sonnet))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "to tie up envy , evermore enlarged ,\n",
                        "my love is a babe , then she\n",
                        "and to flatterer stopped are :\n",
                        "make sweet some vial ; treasure thou some\n",
                        "and you but one hour mine ,\n",
                        "is lust in action , lust\n",
                        "give warning to the lark at break of\n",
                        "when your countenance filled up his burning head\n",
                        "you are you whose worthiness gives scope ,\n",
                        "eat up thy charge ? is this ,\n",
                        "and even thence thou wilt leave me ,\n",
                        "he of tall building , and my discourse\n",
                        "that i was thy will ,\n",
                        "spending again what is had , having ,\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Now let's train a Hidden Markov Model (HMM) that is able to tag words with their part-of-speech (POS)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "def tag_sent(data_file, label=True):\n",
                "    \"\"\"\n",
                "    Load tokens (and labels, if allowed) from a data_file\n",
                "    \"\"\"\n",
                "    with open(f\"assets/conll03/{data_file}\", \"r\") as file_object:\n",
                "        \n",
                "        # read file content and stripp the docstart lines\n",
                "        data = [line for line in file_object.readlines() if line != '-DOCSTART- -X- O O\\n']\n",
                "        # grouping by line break\n",
                "        sentences = [list(g) for k, g in groupby(data, key=lambda x: x != \"\\n\") if k]\n",
                "        # filtering for lines with >= 10 tokens\n",
                "        g10_sent = [line for line in sentences if len(line) >=10]\n",
                "        # nested list comprehension to remove line break\n",
                "        output = [[l.strip('\\n') for l in line] for line in g10_sent]\n",
                "        \n",
                "        \n",
                "        # extract POS if label is True\n",
                "        if label == True:\n",
                "            tagged_sents = [[(t.split()[0],t.split()[1]) for t in line] for line in output]\n",
                "        else:\n",
                "            tagged_sents = [[t.split()[0] for t in line] for line in output]    \n",
                "    \n",
                "    return tagged_sents"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "The data below comes from [CoNLL-2003 shared task](https://www.clips.uantwerpen.be/conll2003/ner/). The shared task was originally held as a competition for Named Entity Recognition (NER), but the data it provided also includes POS tags that make POS Tagging possible. NER and POS Tagging are very similar tasks and HMMs are capable of handling both of them."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# Let's create a training, development and testing sets using tag_sent above\n",
                "# in all the three data files, we only consider \"substantial\" sentences that have at least 10 tokens\n",
                "dataset = {\"train\": tag_sent('eng.train', label=True), \"dev\": tag_sent('eng.testa', label=True), \"test\": tag_sent('eng.testb', label=False)}"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "# Train an HMM tagger, which takes a while\n",
                "hmm_tagger = HiddenMarkovModelTagger.train(dataset[\"train\"], test_sequence = dataset[\"dev\"])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "accuracy over 43319 tokens: 89.13\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now that we have trained an HMM tagger, let's now apply it to the testing set to see how it does. For example, the line of code below tags the first sentence in the testing set."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "hmm_tagger.tag(dataset[\"test\"][0])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[('SOCCER', 'NN'),\n",
                            " ('-', ':'),\n",
                            " ('JAPAN', 'NNP'),\n",
                            " ('GET', '.'),\n",
                            " ('LUCKY', '\"'),\n",
                            " ('WIN', 'NNP'),\n",
                            " (',', ','),\n",
                            " ('CHINA', 'NNP'),\n",
                            " ('IN', 'IN'),\n",
                            " ('SURPRISE', 'NNP'),\n",
                            " ('DEFEAT', 'NNP'),\n",
                            " ('.', '.')]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 17
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}