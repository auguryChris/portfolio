{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Demonstrating how a non-Spark script (NLTK) can be translated to work in a Spark RDD environment.</p><p>Part-of-speech tagging and a super-gentle introduction to Natural Language Processing (NLP). The original script was written by Luke Petschauer and a forked version is available at https://github.com/umsi-data-science/NP_chunking_with_nltk/blob/master/NP_chunking_with_the_NLTK.ipynb. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book')\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The next cell is the original (non-Spark) script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stellar pitching', 'afloat', 'first half', 'last season', 'encore', 'pennant-winning season', 'lineup', 'pitching', 'thin', 'pitching', 's game', 'pitching', 'anything', '4-2 loss', 'place', 'spot starter', 'deficit', 'lineup', 'starter', 'room', 'ninth inning', 'last-gasp two-run homer', 'reliever', 'streak', 'team']\n"
     ]
    }
   ],
   "source": [
    "# This is the original (non-Spark) script\n",
    "\n",
    "patterns = \"\"\"\n",
    "    NP: {<JJ>*<NN*>+}\n",
    "    {<JJ>*<NN*><CC>*<NN*>+}\n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "\n",
    "def sent_parse(input):\n",
    "    sentences = prepare_text(str(input))\n",
    "    nps = parsed_text_to_NP(sentences)\n",
    "    return nps\n",
    "\n",
    "\n",
    "text_to_be_analyzed = \"\"\"WASHINGTON - Stellar pitching kept the Mets afloat in the first half of last season despite their offensive woes. But they cannot produce an encore of their pennant-winning season if their lineup keeps floundering while their pitching is nicked, bruised and stretched thin.\n",
    "\"We were going to ride our pitching,\" Manager Terry Collins said before Wednesday’s game. \"But we're not riding it right now. We've got as many problems with our pitching as we do anything.\"\n",
    "Wednesday's 4-2 loss to the Washington Nationals was cruel for the already-limping Mets. Pitching in Steven Matz's place, the spot starter Logan Verrett allowed two runs over five innings. But even that was too large a deficit for the Mets' lineup to overcome against Max Scherzer, the Nationals' starter.\n",
    "\"We're not even giving ourselves chances,\" Collins said, adding later, \"We just can’t give our pitchers any room to work.\"\n",
    "The Mets did not score until the ninth inning, when a last-gasp two-run homer by James Loney off Nationals reliever Shawn Kelley snapped a streak of 23 scoreless innings for the team.\"\"\"\n",
    "\n",
    "\n",
    "nps = sent_parse(text_to_be_analyzed)\n",
    "print(nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('Sparky') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL: http://www.nytimes.com/2016/06/30/sports/baseball/washington-nationals-max-scherzer-baffles-mets-completing-a-sweep.html',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile('data/nytimes/nytimes_news_articles.txt')\n",
    "# show the first two lines of the file\n",
    "text.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"\\b[\\w']+\\b\")\n",
    "def pos_tag_counter(line):\n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    return postoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an RDD pipline\n",
    "\n",
    "1. filters out blank lines \n",
    "2. filters out lines starting with 'URL'\n",
    "3. creates a single list (using flatMap) that applies the pos_tag_counter function to each line\n",
    "4. maps each resulting line to show the part of speech (which is the second element returned from the pos_tag_counter)\n",
    "5. converts each resulting line to a pairRDD with POS tags as keys and values of 1\n",
    "6. reduces the resulting RDD by key, adding up all the 1s (like the lecture and lab examples)\n",
    "7. sorts the resulting list by the counts, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 1126515),\n",
       " ('IN', 928916),\n",
       " ('NNP', 853093),\n",
       " ('DT', 761492),\n",
       " ('JJ', 498482),\n",
       " ('NNS', 437116),\n",
       " ('VBD', 379509),\n",
       " ('PRP', 282603),\n",
       " ('RB', 271053),\n",
       " ('CC', 231491),\n",
       " ('VB', 223717),\n",
       " ('CD', 187602),\n",
       " ('TO', 187005),\n",
       " ('VBN', 174980),\n",
       " ('VBZ', 169149),\n",
       " ('VBG', 163653),\n",
       " ('VBP', 143368),\n",
       " ('PRP$', 107984),\n",
       " ('MD', 67185),\n",
       " ('WDT', 44582),\n",
       " ('WP', 42406),\n",
       " ('WRB', 33160),\n",
       " ('RP', 29345),\n",
       " ('JJR', 24746),\n",
       " ('NNPS', 18870),\n",
       " ('JJS', 16425),\n",
       " ('EX', 12397),\n",
       " ('RBR', 12286),\n",
       " ('RBS', 5146),\n",
       " ('PDT', 3784),\n",
       " ('FW', 2793),\n",
       " ('WP$', 2329),\n",
       " ('POS', 493),\n",
       " ('UH', 325),\n",
       " ('$', 219),\n",
       " ('LS', 5),\n",
       " (\"''\", 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_flt = text.filter(lambda x: not re.match(r'^URL', x))\n",
    "text_flt = text_flt.filter(lambda x: not re.match(r'^\\s*$', x))\n",
    "mapped = text_flt.flatMap(lambda line: pos_tag_counter(line))\n",
    "keyed = mapped.map(lambda pos: (pos[1],1))\n",
    "wc = keyed.reduceByKey(lambda accumulator,value: accumulator + value)\n",
    "last_step = wc.sortByKey(lambda x,y: x+y)\n",
    "last_step.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJS>*<NN.*>}\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "def tokenize_chunk_parse(line):\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "  \n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    tree = chunker.parse(postoks)\n",
    "\n",
    "    return [term for term in leaves(tree)] \n",
    "  \n",
    "def leaves(tree):\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an RDD pipeline to show the distribution of the length of noun phrases\n",
    "\n",
    "1. Apply (using flatMap) the tokenize_chunk_parse function to each line in the text RDD \n",
    "2. Use map to emit the length of each noun phrase \n",
    "3. Use map to convert each resulting line to a pairRDD with lengths of noun phrases as keys and values of 1 \n",
    "4. Reduce the resulting RDD by key, adding up all the 1s (like the lecture and lab examples) \n",
    "5. Sort the resulting list by the counts, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1194014),\n",
       " ('2', 345011),\n",
       " ('3', 106957),\n",
       " ('4', 34459),\n",
       " ('5', 10561),\n",
       " ('6', 3638),\n",
       " ('7', 1261),\n",
       " ('8', 494),\n",
       " ('9', 238),\n",
       " ('10', 106),\n",
       " ('11', 50),\n",
       " ('13', 34),\n",
       " ('12', 26),\n",
       " ('14', 23),\n",
       " ('16', 16),\n",
       " ('18', 14),\n",
       " ('27', 10),\n",
       " ('19', 9),\n",
       " ('20', 9),\n",
       " ('32', 9),\n",
       " ('15', 8),\n",
       " ('34', 8),\n",
       " ('17', 7),\n",
       " ('24', 7),\n",
       " ('25', 7),\n",
       " ('26', 7),\n",
       " ('40', 7),\n",
       " ('21', 6),\n",
       " ('28', 6),\n",
       " ('37', 6),\n",
       " ('46', 6),\n",
       " ('22', 5),\n",
       " ('23', 5),\n",
       " ('29', 5),\n",
       " ('30', 4),\n",
       " ('31', 4),\n",
       " ('33', 4),\n",
       " ('39', 4),\n",
       " ('41', 4),\n",
       " ('44', 4),\n",
       " ('55', 4),\n",
       " ('36', 3),\n",
       " ('43', 3),\n",
       " ('45', 3),\n",
       " ('48', 3),\n",
       " ('49', 3),\n",
       " ('50', 3),\n",
       " ('51', 3),\n",
       " ('56', 3),\n",
       " ('57', 3),\n",
       " ('63', 3),\n",
       " ('71', 3),\n",
       " ('35', 2),\n",
       " ('47', 2),\n",
       " ('61', 2),\n",
       " ('65', 2),\n",
       " ('104', 1),\n",
       " ('113', 1),\n",
       " ('127', 1),\n",
       " ('131', 1),\n",
       " ('135', 1),\n",
       " ('140', 1),\n",
       " ('38', 1),\n",
       " ('42', 1),\n",
       " ('53', 1),\n",
       " ('54', 1),\n",
       " ('58', 1),\n",
       " ('64', 1),\n",
       " ('66', 1),\n",
       " ('75', 1),\n",
       " ('80', 1),\n",
       " ('82', 1),\n",
       " ('88', 1),\n",
       " ('91', 1),\n",
       " ('92', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_flt is the filtered text without URL and empty lines\n",
    "st_1 = text_flt.flatMap(lambda line: tokenize_chunk_parse(line))  \n",
    "st_2 = st_1.map(lambda chunk: len(chunk))\n",
    "st_3 = st_2.map(lambda count: (str(count),1))\n",
    "st_4 = st_3.reduceByKey(lambda accumulator,value: accumulator + value)\n",
    "st_5 = st_4.sortByKey(lambda x,y: x+y)\n",
    "st_5.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
